{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_Lambda = 0.9\n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "M = 30\n",
    "drop_out = 0.2\n",
    "first_dense_layer_nodes  = 256\n",
    "second_dense_layer_nodes = 2 #second dense layer would be the output\n",
    "IsSynthetic = False\n",
    "PHI = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data from same_pairs.csv and diff_pairs.csv and merge them after sampling random points from diff_pairs.csv to reduce length\n",
    "def readCsvAndConcatData(filename1, filename2, filename3):\n",
    "    same_df = pd.read_csv(filename1)\n",
    "    s = len(same_df)\n",
    "    n = sum(1 for line in open(filename2))\n",
    "    print(n)\n",
    "    skip = sorted(rd.sample(range(1,n),(n-(s+1)))) #the 0-indexed header will not be included in the skip list\n",
    "    #print(skip)\n",
    "    diff_df = pd.read_csv(filename2,skiprows=skip)\n",
    "    #print(\"Length of the same_df is\", len(same_df))\n",
    "    #print(\"Length of the diff_df is\", len(diff_df))\n",
    "    \n",
    "    concat_df = pd.concat([same_df,diff_df])\n",
    "\n",
    "    #print(concat_df)\n",
    "    #print(len(same_df.columns))\n",
    "    #print(len(diff_df.columns))\n",
    "    \n",
    "    features_df = pd.read_csv(filename3)\n",
    "    features_df= features_df.iloc[:,1:]\n",
    "    #features_df = features_df.iloc[:,1:]\n",
    "    #print(features_df_new)\n",
    "    new_df = concat_df.join(features_df.set_index([ 'img_id' ], verify_integrity=True), on=[ 'img_id_A' ], how='left')\n",
    "    new_df = new_df.join(features_df.set_index([ 'img_id' ], verify_integrity=True) , on=[ 'img_id_B' ], how='left',lsuffix='_left', rsuffix='_right')\n",
    "    #print(new_df)\n",
    "    #print(\"Successfully concatenated the file fetching its features\")\n",
    "    return new_df\n",
    "\n",
    "#GSC data has one less column and more number of rows thus new function defined to accomodate these changes in the csv.\n",
    "def readCsvAndConcatDataGSC(filename1, filename2, filename3):\n",
    "    same_df = pd.read_csv(filename1)\n",
    "    s = len(same_df)\n",
    "    n = sum(1 for line in open(filename2))\n",
    "    #print(n)\n",
    "    skip = sorted(rd.sample(range(1,n),(n-(s+1)))) #the 0-indexed header will not be included in the skip list\n",
    "    #print(skip)\n",
    "    diff_df = pd.read_csv(filename2,skiprows=skip)\n",
    "    #print(\"Length of the same_df is\", len(same_df))\n",
    "    #print(\"Length of the diff_df is\", len(diff_df))\n",
    "    \n",
    "    concat_df = pd.concat([same_df,diff_df])\n",
    "\n",
    "    #print(concat_df)\n",
    "    #print(len(same_df.columns))\n",
    "    #print(len(diff_df.columns))\n",
    "    \n",
    "    features_df_new = pd.read_csv(filename3)\n",
    "    features_df_new2 = pd.read_csv(filename3)\n",
    "    #features_df_new = features_df.iloc[:,1:]\n",
    "    #features_df_new2 = features_df.iloc[:,1:]\n",
    "    #print(features_df_new)\n",
    "    new_df = concat_df.join(features_df_new.set_index([ 'img_id' ], verify_integrity=True), on=[ 'img_id_A' ], how='left')\n",
    "    new_df = new_df.join(features_df_new2.set_index([ 'img_id' ], verify_integrity=True) , on=[ 'img_id_B' ], how='left',lsuffix='_left', rsuffix='_right')\n",
    "    #print(new_df)\n",
    "    #print(\"Successfully concatenated the file fetching its features\")\n",
    "    return new_df\n",
    "\n",
    "#Training target would be 80% of the total data\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "#Training data would be 80% of the total data\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "#Validation data would be 10% of the total data\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "#Validation target vector would be 10% of the entire target vector\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for passing the human observed data for concatenation\n",
    "def CreateHumanObservedDataConcat():\n",
    "    #tableDataConcat = readCsvAndConcatData(os.path.join(\"Human_Observed_Data\",\"same_pairs.csv\"), os.path.join(\"Human_Observed_Data\",\"diffn_pairs.csv\"), os.path.join(\"Human_Observed_Data\",\"HumanObserved-Features-Data.csv\"))\n",
    "    tableDataConcat = readCsvAndConcatData(\"./Human_Observed_Data/same_pairs.csv\", \"./Human_Observed_Data/diffn_pairs.csv\", \"./Human_Observed_Data/HumanObserved-Features-Data.csv\")\n",
    "    #tableDataConcat = readCsvAndConcatData(\"same_pairs.csv\", \"diffn_pairs.csv\", \"HumanObserved-Features-Data.csv\")\n",
    "    #print(\"====================================The appended data is given by===============================================\\n\")\n",
    "    #print(tableDataConcat.loc[[1]])\n",
    "    return tableDataConcat\n",
    "\n",
    "#Function for passing the human observed data for subtraction\n",
    "def CreateHumanObservedDataSub(tableDataConcat):\n",
    "    tableDataF1List = tableDataConcat.iloc[:,3:12]\n",
    "    #print(\"=============F1 LIST==============\")\n",
    "    #print(tableDataF1List)\n",
    "    tableDataF2List = tableDataConcat.iloc[:,12:22]\n",
    "    #print(\"=============F2 LIST==============\")\n",
    "    #print(tableDataF2List)\n",
    "    tableDataSub = tableDataF1List.sub(tableDataF2List.values)\n",
    "    tableDataSub['target']= tableDataConcat['target']\n",
    "    #print(\"=====================================The subtracted data is given by============================================\")\n",
    "    #print(tableDataSub)\n",
    "    return tableDataSub\n",
    "    \n",
    "#Function for passing the GSC data for concatenation    \n",
    "def CreateGSCDataConcat():\n",
    "    tableDataConcat = readCsvAndConcatDataGSC(\"./GSC_Data/same_pairs.csv\", \"./GSC_Data/diffn_pairs.csv\",\"./GSC_Data/GSC-Features.csv\")\n",
    "    return tableDataConcat\n",
    "\n",
    "#Function for passing the GSC data for subtraction\n",
    "def CreateGSCDataSub(tableDataConcat):\n",
    "    tableDataF1List = tableDataConcat.iloc[:,3:515]\n",
    "    #print(\"=============F1 LIST==============\")\n",
    "    #print(tableDataF1List)\n",
    "    tableDataF2List = tableDataConcat.iloc[:,515:1027]\n",
    "    #print(\"=============F2 LIST==============\")\n",
    "    #print(tableDataF2List)\n",
    "    tableDataSub = abs(tableDataF1List.sub(tableDataF2List.values))\n",
    "    tableDataSub['target']= tableDataConcat['target']\n",
    "    #print(\"=====================================The subtracted data is given by============================================\")\n",
    "    return tableDataSub\n",
    "\n",
    "#Model made for Neural Network\n",
    "def get_model():\n",
    "    \n",
    "    # Why do we need a model? -> ML would require the construction of a neural network, which has an input layer,\n",
    "    # one or more hidden layers and an output layer. By choosing appropriate features for this model, we would be able to \n",
    "    # build its decision making capability\n",
    "    # Why use Dense layer and then activation? -> Usage of dense layer suggests that each input is connected to each output within\n",
    "    # its layer. Since FizzBuzz is a classification problem, such a connectivity is required for us. \n",
    "    # Why use sequential model with layers? -> Sequential model is used since for every input there is exactly one possible output.\n",
    "    # The layers have been added (you would know why when you read about relu and softmax)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size )) #Since this is the layer which is directly connected to the\n",
    "    # input layer, the input size needs to be specified. For subsequent layers, it will take this value from the output of the \n",
    "    # previous layer\n",
    "    #model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Why dropout?\n",
    "    model.add(Dropout(drop_out)) #Dropout is a technique to reduce the dependency of the model on a particular neuron,\n",
    "    # by dropping out random neuron values during the training. It is a model de-sensitizing technique\n",
    "    \n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('softmax'))\n",
    "    # Why Softmax? -> Softmax is used for classification problems since it associates a probability value with each of the output\n",
    "    # possibilities. The output with the highest probability would be chosen as the final output.\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Why use categorical_crossentropy? -> Categorical crossentropy would be used as the cost function, in order to estimate how\n",
    "    # FAR we are from the actual result. If the predicted probability is closer to the actual probability, then the value of the \n",
    "    # cost will go on decreasing\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking in User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in 1 for Human Observed Dataset or 2 for GSC Dataset1\n",
      "Type in 1 for concatenated data and 2 for subtracted data2\n",
      "===========================================HumanObservedDataConcat=============================================\n",
      "293033\n",
      "===========================================HumanObservedDataSub=============================================\n",
      "(1582,)\n",
      "(1582, 9)\n",
      "Type in 1 for Linear regression and 2 for Logistic regression and 3 for Neural Network3\n"
     ]
    }
   ],
   "source": [
    "user_option = input(\"Type in 1 for Human Observed Dataset or 2 for GSC Dataset\")\n",
    "\n",
    "if user_option == \"1\":\n",
    "    user_option_2 = input(\"Type in 1 for concatenated data and 2 for subtracted data\")\n",
    "    print(\"===========================================HumanObservedDataConcat=============================================\")\n",
    "    HumanObservedDataConcat = shuffle(CreateHumanObservedDataConcat())\n",
    "    if user_option_2 == \"1\":\n",
    "        HumanObservedDataConcat = HumanObservedDataConcat.loc[:, (HumanObservedDataConcat != 0).any(axis=0)]\n",
    "        DataTarget = np.array(HumanObservedDataConcat.iloc[:,2]).flatten()\n",
    "        DataNP = np.array(HumanObservedDataConcat.iloc[:,3:])\n",
    "        print(DataTarget.shape)\n",
    "        print(DataNP.shape)\n",
    "\n",
    "    if user_option_2 == \"2\":\n",
    "        print(\"===========================================HumanObservedDataSub=============================================\")\n",
    "        HumanObservedDataSub = shuffle(CreateHumanObservedDataSub(HumanObservedDataConcat))\n",
    "        HumanObservedDataSub = HumanObservedDataSub.loc[:, (HumanObservedDataSub != 0).any(axis=0)]\n",
    "        DataTarget = np.array(HumanObservedDataSub.iloc[:,len(HumanObservedDataSub.columns) - 1]).flatten()\n",
    "        DataNP = np.array(HumanObservedDataSub.iloc[:,:len(HumanObservedDataSub.columns) - 1])\n",
    "        print(DataTarget.shape)\n",
    "        print(DataNP.shape)\n",
    "        M = len(DataNP.shape)\n",
    "\n",
    "elif user_option == \"2\":\n",
    "    user_option_2 = input(\"Type in 1 for concatenated data and 2 for subtracted data\")\n",
    "    print(\"=============================================GSCDataConcat=========================================================\")\n",
    "    GSCDataConcat = shuffle(CreateGSCDataConcat())\n",
    "\n",
    "    if user_option_2 == \"1\":\n",
    "        GSCDataConcat = GSCDataConcat.loc[:, (GSCDataConcat != 0).any(axis=0)]\n",
    "        DataTarget = np.array(GSCDataConcat.iloc[:,2]).flatten()\n",
    "        DataNP = np.array(GSCDataConcat.iloc[:,3:])\n",
    "        print(DataTarget.shape)\n",
    "        print(DataNP.shape)\n",
    "    if user_option_2 == \"2\":\n",
    "        print(\"=============================================GSCDataSub============================================\")\n",
    "        GSCDataSub = shuffle(CreateGSCDataSub(GSCDataConcat))\n",
    "        print(GSCDataSub.shape)\n",
    "        #Eliminate the features that have value 0 for all data points\n",
    "        GSCDataSub=GSCDataSub.loc[:, (GSCDataSub != 0).any(axis=0)]\n",
    "        print(GSCDataSub.shape)\n",
    "        DataTarget = np.array(GSCDataSub.iloc[:,len(GSCDataSub.columns) - 1 ]).flatten()\n",
    "        DataNP = np.array(GSCDataSub.iloc[:,:len(GSCDataSub.columns) - 1 ])\n",
    "        print(DataTarget.shape)\n",
    "        print(DataNP.shape)\n",
    "\n",
    "DataNP = np.transpose(DataNP)\n",
    "\n",
    "user_option_3 = input(\"Type in 1 for Linear regression and 2 for Logistic regression and 3 for Neural Network\")\n",
    "\n",
    "if user_option_3 == \"2\":\n",
    "    M = len(DataNP)\n",
    "\n",
    "elif user_option_3 == \"3\":\n",
    "    input_size = len(DataNP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1266,)\n",
      "(9, 1266)\n"
     ]
    }
   ],
   "source": [
    "#The training target would be 80% of the total given data\n",
    "TrainingTarget = np.array(GenerateTrainingTarget(DataTarget,TrainingPercent))\n",
    "#The training data would be 80% of the entire training data\n",
    "TrainingData   = GenerateTrainingDataMatrix(DataNP,TrainingPercent)\n",
    "#shape function in numpy used to return the shape of the numpy array\n",
    "print(TrainingTarget.shape)\n",
    "print(TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Val Target Data Generated..\n",
      "10% Val Data Generated..\n",
      "(158,)\n",
      "(9, 158)\n"
     ]
    }
   ],
   "source": [
    "#The validation target would be 10% of the total given target\n",
    "ValDataAct = np.array(GenerateValTargetVector(DataTarget,ValidationPercent, (len(TrainingTarget))))\n",
    "#The validation data would be 10% of the total given data\n",
    "ValData    = GenerateValData(DataNP,ValidationPercent, (len(TrainingTarget)))\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% Val Target Data Generated..\n",
      "10% Val Data Generated..\n",
      "(158,)\n",
      "(9, 158)\n"
     ]
    }
   ],
   "source": [
    "#The test target would be 10% of the total given target\n",
    "TestDataAct = np.array(GenerateValTargetVector(DataTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "#The test data would be 10% of the total given data\n",
    "TestData = GenerateValData(DataNP,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])\n",
    "        #varVect will contain the co-variance value for each data point (sigma squarred)\n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "    #Converting the co-variance value to a matrix of BigSigma values that would be used in the gaussian radial basis function\n",
    "    #formula\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "  \n",
    "    #Multiply the bigsigma value(variance) with a constant of 200 which will cause the points to be more spread out, but not\n",
    "    #cause any difference to the end result\n",
    "    BigSigma = np.dot(200,BigSigma)\n",
    "    print (\"BigSigma Generated..\")\n",
    "    print (\"==============================================BIG SIGMA===========================================================\")\n",
    "    print(BigSigma)\n",
    "    return BigSigma\n",
    "\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):\n",
    "    #DataRow : 1x41 MuRow:1x41 which will make R:1x41 as well\n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    #BigSigInv: 41x41 np.transpose(R): 41x1 which will make T: 41x1\n",
    "    T = np.dot(BigSigInv,np.transpose(R))\n",
    "    #Since we need the squarred values, multiply again. R: 1x41 T:41x1 which will result in a scalar value\n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    # equivalent to one term in the matrix PHI[R][C]\n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))  \n",
    "    #making a nested array with values as all zeroes, having dimension of 10(number of features) x training percent of the data\n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    #The BigSig value is inversed since it has to be brought to the numerator so that it can be multiplied with the other matrices\n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)): #len(MuMatrix) will return the number of rows that are present in this matrix i.e. 10\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            #For the entire training length, (0.8 * 69623) the value will be subtracted from centroid value given by the MuMatrix\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def GetValTestLinear(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    Y = 1/(1+ np.exp(-Y))\n",
    "    return Y\n",
    "\n",
    "def GetErmsLinear(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        #Getting the sum of squarred error values\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    #getting the percent accuracy\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    #Returns an appended string containing the accuracy and Erms error values\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "\n",
    "def encodeLabel(TrainingTarget):\n",
    "    processedLabel = []\n",
    "    for trainingValue in TrainingTarget:\n",
    "        if(trainingValue == 0):\n",
    "            processedLabel.append([0])\n",
    "        elif(trainingValue == 1):\n",
    "            processedLabel.append([1])\n",
    "    return np_utils.to_categorical(np.array(processedLabel),2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_option_3 == \"1\" or user_option_3 == \"2\":\n",
    "    #Moore penrose pseudo inverse is very close to being the inverse of the matrix, with lesser computation time. Hence this is the\n",
    "    #choice of inversion function that is used here\n",
    "    ErmsArr = []\n",
    "    AccuracyArr = []\n",
    "\n",
    "    #kmeans is a clustering algorithm which is used in order to cluster the points that are closer together into groups.\n",
    "    #This function is a part of the sklearn.cluster provider, the parameters given are the number of clusters (10) and the random\n",
    "    #option will choose the centroids in the beginning at random. It performs 300 iterations for clustering by default, through which\n",
    "    #it decides which cluster each data point should reside in. The data that is passed is the training data through the fit function\n",
    "    #which tells us that using the given k-means configuration, fit the TrainingData into it.\n",
    "    kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData))\n",
    "\n",
    "    #Mu is a 10 x 41 array which will contain the centroids for each of the 41 features over the entire data set.\n",
    "    Mu = kmeans.cluster_centers_\n",
    "    print(Mu)\n",
    "    BigSigma     = GenerateBigSigma(DataNP, Mu, TrainingPercent)\n",
    "    TRAINING_PHI = GetPhiMatrix(DataNP, Mu, BigSigma, TrainingPercent)\n",
    "    #Weights calculated only on the basis of the training data\n",
    "    W            = np.random.rand(M)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++Printing W++++++++++++++++++++++++++++++++++++\")\n",
    "    print(W)\n",
    "    TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100) \n",
    "    VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_option_3 == \"1\" or user_option_3 == \"2\":\n",
    "    print(Mu.shape)\n",
    "    print(BigSigma.shape)\n",
    "    print(TRAINING_PHI.shape)\n",
    "    print(W.shape)\n",
    "    print(VAL_PHI.shape)\n",
    "    print(TEST_PHI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 256)               2560      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,074\n",
      "Trainable params: 3,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Prepared model\n",
      "1266\n",
      "Train on 1012 samples, validate on 254 samples\n",
      "Epoch 1/10000\n",
      "1012/1012 [==============================] - 0s 170us/step - loss: 0.7449 - acc: 0.5079 - val_loss: 0.6937 - val_acc: 0.5394\n",
      "Epoch 2/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7040 - acc: 0.5267 - val_loss: 0.6966 - val_acc: 0.4685\n",
      "Epoch 3/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.7370 - acc: 0.5040 - val_loss: 0.7230 - val_acc: 0.4961\n",
      "Epoch 4/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7092 - acc: 0.5247 - val_loss: 0.7225 - val_acc: 0.4961\n",
      "Epoch 5/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.7139 - acc: 0.5247 - val_loss: 0.7201 - val_acc: 0.5039\n",
      "Epoch 6/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.7175 - acc: 0.5059 - val_loss: 0.7156 - val_acc: 0.5118\n",
      "Epoch 7/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7208 - acc: 0.5089 - val_loss: 0.7042 - val_acc: 0.4843\n",
      "Epoch 8/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7247 - acc: 0.5059 - val_loss: 0.7040 - val_acc: 0.4961\n",
      "Epoch 9/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.7251 - acc: 0.4901 - val_loss: 0.7053 - val_acc: 0.4646\n",
      "Epoch 10/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7223 - acc: 0.4951 - val_loss: 0.7212 - val_acc: 0.5197\n",
      "Epoch 11/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.7307 - acc: 0.5099 - val_loss: 0.7132 - val_acc: 0.4921\n",
      "Epoch 12/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7217 - acc: 0.5089 - val_loss: 0.7098 - val_acc: 0.4843\n",
      "Epoch 13/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7236 - acc: 0.5040 - val_loss: 0.7028 - val_acc: 0.4843\n",
      "Epoch 14/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7147 - acc: 0.5178 - val_loss: 0.7421 - val_acc: 0.5039\n",
      "Epoch 15/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.7176 - acc: 0.5089 - val_loss: 0.7300 - val_acc: 0.5079\n",
      "Epoch 16/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7163 - acc: 0.5227 - val_loss: 0.7239 - val_acc: 0.5000\n",
      "Epoch 17/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.7178 - acc: 0.5138 - val_loss: 0.7059 - val_acc: 0.4528\n",
      "Epoch 18/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.7115 - acc: 0.5306 - val_loss: 0.7548 - val_acc: 0.5039\n",
      "Epoch 19/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7183 - acc: 0.5188 - val_loss: 0.7156 - val_acc: 0.4882\n",
      "Epoch 20/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.7152 - acc: 0.5168 - val_loss: 0.7121 - val_acc: 0.4843\n",
      "Epoch 21/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7164 - acc: 0.5188 - val_loss: 0.7169 - val_acc: 0.5000\n",
      "Epoch 22/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7274 - acc: 0.4743 - val_loss: 0.7103 - val_acc: 0.4961\n",
      "Epoch 23/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6999 - acc: 0.5435 - val_loss: 0.7413 - val_acc: 0.5000\n",
      "Epoch 24/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7132 - acc: 0.5257 - val_loss: 0.7111 - val_acc: 0.5000\n",
      "Epoch 25/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7114 - acc: 0.5158 - val_loss: 0.7532 - val_acc: 0.5039\n",
      "Epoch 26/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7084 - acc: 0.5247 - val_loss: 0.7427 - val_acc: 0.5039\n",
      "Epoch 27/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7175 - acc: 0.5158 - val_loss: 0.7044 - val_acc: 0.4961\n",
      "Epoch 28/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7058 - acc: 0.5227 - val_loss: 0.7180 - val_acc: 0.5079\n",
      "Epoch 29/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7126 - acc: 0.5119 - val_loss: 0.7041 - val_acc: 0.4803\n",
      "Epoch 30/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7039 - acc: 0.5346 - val_loss: 0.7228 - val_acc: 0.5039\n",
      "Epoch 31/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7187 - acc: 0.5069 - val_loss: 0.7278 - val_acc: 0.5039\n",
      "Epoch 32/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7207 - acc: 0.4901 - val_loss: 0.7160 - val_acc: 0.5157\n",
      "Epoch 33/10000\n",
      "1012/1012 [==============================] - 0s 20us/step - loss: 0.7170 - acc: 0.4980 - val_loss: 0.7297 - val_acc: 0.4961\n",
      "Epoch 34/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7113 - acc: 0.5267 - val_loss: 0.7453 - val_acc: 0.5118\n",
      "Epoch 35/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7139 - acc: 0.5099 - val_loss: 0.7101 - val_acc: 0.4882\n",
      "Epoch 36/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7101 - acc: 0.5079 - val_loss: 0.7051 - val_acc: 0.4882\n",
      "Epoch 37/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7108 - acc: 0.5158 - val_loss: 0.7120 - val_acc: 0.5079\n",
      "Epoch 38/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7103 - acc: 0.5040 - val_loss: 0.7788 - val_acc: 0.5000\n",
      "Epoch 39/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7151 - acc: 0.5049 - val_loss: 0.7126 - val_acc: 0.5118\n",
      "Epoch 40/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7082 - acc: 0.5109 - val_loss: 0.7417 - val_acc: 0.4961\n",
      "Epoch 41/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7189 - acc: 0.4960 - val_loss: 0.7077 - val_acc: 0.4961\n",
      "Epoch 42/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7017 - acc: 0.5425 - val_loss: 0.7456 - val_acc: 0.5000\n",
      "Epoch 43/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7065 - acc: 0.5138 - val_loss: 0.7217 - val_acc: 0.5039\n",
      "Epoch 44/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6991 - acc: 0.5366 - val_loss: 0.7304 - val_acc: 0.5039\n",
      "Epoch 45/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.7133 - acc: 0.5148 - val_loss: 0.7125 - val_acc: 0.4843\n",
      "Epoch 46/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7021 - acc: 0.5296 - val_loss: 0.7702 - val_acc: 0.4961\n",
      "Epoch 47/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7094 - acc: 0.5168 - val_loss: 0.7082 - val_acc: 0.5000\n",
      "Epoch 48/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7109 - acc: 0.4901 - val_loss: 0.7490 - val_acc: 0.5000\n",
      "Epoch 49/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7093 - acc: 0.5168 - val_loss: 0.7266 - val_acc: 0.4961\n",
      "Epoch 50/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7137 - acc: 0.5040 - val_loss: 0.7046 - val_acc: 0.5039\n",
      "Epoch 51/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6971 - acc: 0.5287 - val_loss: 0.7406 - val_acc: 0.5039\n",
      "Epoch 52/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7162 - acc: 0.5040 - val_loss: 0.7662 - val_acc: 0.5039\n",
      "Epoch 53/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7129 - acc: 0.5178 - val_loss: 0.7241 - val_acc: 0.4921\n",
      "Epoch 54/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7079 - acc: 0.5010 - val_loss: 0.7090 - val_acc: 0.4882\n",
      "Epoch 55/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6987 - acc: 0.5306 - val_loss: 0.7029 - val_acc: 0.4843\n",
      "Epoch 56/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6940 - acc: 0.5326 - val_loss: 0.7245 - val_acc: 0.4921\n",
      "Epoch 57/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7037 - acc: 0.5296 - val_loss: 0.7494 - val_acc: 0.5039\n",
      "Epoch 58/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7100 - acc: 0.4980 - val_loss: 0.7422 - val_acc: 0.5079\n",
      "Epoch 59/10000\n",
      "1012/1012 [==============================] - 0s 20us/step - loss: 0.7071 - acc: 0.5158 - val_loss: 0.7208 - val_acc: 0.5000\n",
      "Epoch 60/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6917 - acc: 0.5415 - val_loss: 0.7238 - val_acc: 0.5000\n",
      "Epoch 61/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.7040 - acc: 0.5217 - val_loss: 0.7082 - val_acc: 0.4961\n",
      "Epoch 62/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6994 - acc: 0.5109 - val_loss: 0.7285 - val_acc: 0.4882\n",
      "Epoch 63/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7008 - acc: 0.5158 - val_loss: 0.7470 - val_acc: 0.5039\n",
      "Epoch 64/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.7029 - acc: 0.5316 - val_loss: 0.7363 - val_acc: 0.5000\n",
      "Epoch 65/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.7085 - acc: 0.5178 - val_loss: 0.7206 - val_acc: 0.4882\n",
      "Epoch 66/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7004 - acc: 0.5306 - val_loss: 0.7140 - val_acc: 0.5118\n",
      "Epoch 67/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7120 - acc: 0.5010 - val_loss: 0.7103 - val_acc: 0.4961\n",
      "Epoch 68/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7053 - acc: 0.5069 - val_loss: 0.7022 - val_acc: 0.4685\n",
      "Epoch 69/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6989 - acc: 0.5138 - val_loss: 0.7304 - val_acc: 0.4921\n",
      "Epoch 70/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7029 - acc: 0.5217 - val_loss: 0.7054 - val_acc: 0.4921\n",
      "Epoch 71/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6973 - acc: 0.5306 - val_loss: 0.7142 - val_acc: 0.5118\n",
      "Epoch 72/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7099 - acc: 0.5030 - val_loss: 0.7039 - val_acc: 0.5079\n",
      "Epoch 73/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7123 - acc: 0.4931 - val_loss: 0.7018 - val_acc: 0.4724\n",
      "Epoch 74/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7081 - acc: 0.5040 - val_loss: 0.7187 - val_acc: 0.4961\n",
      "Epoch 75/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.7050 - acc: 0.5119 - val_loss: 0.7301 - val_acc: 0.5000\n",
      "Epoch 76/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.7019 - acc: 0.5059 - val_loss: 0.7023 - val_acc: 0.4724\n",
      "Epoch 77/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6973 - acc: 0.5405 - val_loss: 0.7108 - val_acc: 0.4882\n",
      "Epoch 78/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6938 - acc: 0.5277 - val_loss: 0.7177 - val_acc: 0.5039\n",
      "Epoch 79/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7051 - acc: 0.5178 - val_loss: 0.7125 - val_acc: 0.4961\n",
      "Epoch 80/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.7010 - acc: 0.5178 - val_loss: 0.7197 - val_acc: 0.4961\n",
      "Epoch 81/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6967 - acc: 0.5188 - val_loss: 0.7561 - val_acc: 0.4961\n",
      "Epoch 82/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7044 - acc: 0.5287 - val_loss: 0.7053 - val_acc: 0.4921\n",
      "Epoch 83/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6943 - acc: 0.5326 - val_loss: 0.7654 - val_acc: 0.5039\n",
      "Epoch 84/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7078 - acc: 0.5188 - val_loss: 0.7079 - val_acc: 0.4803\n",
      "Epoch 85/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6996 - acc: 0.5306 - val_loss: 0.7110 - val_acc: 0.4961\n",
      "Epoch 86/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6992 - acc: 0.5257 - val_loss: 0.7610 - val_acc: 0.4921\n",
      "Epoch 87/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7076 - acc: 0.5168 - val_loss: 0.7188 - val_acc: 0.5039\n",
      "Epoch 88/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6896 - acc: 0.5642 - val_loss: 0.7337 - val_acc: 0.5000\n",
      "Epoch 89/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6990 - acc: 0.5217 - val_loss: 0.7491 - val_acc: 0.5039\n",
      "Epoch 90/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.7100 - acc: 0.4980 - val_loss: 0.7071 - val_acc: 0.4567\n",
      "Epoch 91/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6979 - acc: 0.5128 - val_loss: 0.7057 - val_acc: 0.4843\n",
      "Epoch 92/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7042 - acc: 0.5208 - val_loss: 0.7245 - val_acc: 0.4921\n",
      "Epoch 93/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7056 - acc: 0.5128 - val_loss: 0.7079 - val_acc: 0.5000\n",
      "Epoch 94/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.7021 - acc: 0.5188 - val_loss: 0.7212 - val_acc: 0.5000\n",
      "Epoch 95/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.7018 - acc: 0.5287 - val_loss: 0.7193 - val_acc: 0.5118\n",
      "Epoch 96/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6984 - acc: 0.5198 - val_loss: 0.7119 - val_acc: 0.5039\n",
      "Epoch 97/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6894 - acc: 0.5514 - val_loss: 0.7169 - val_acc: 0.5197\n",
      "Epoch 98/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6989 - acc: 0.5247 - val_loss: 0.7052 - val_acc: 0.4921\n",
      "Epoch 99/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6885 - acc: 0.5504 - val_loss: 0.7045 - val_acc: 0.4646\n",
      "Epoch 100/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6923 - acc: 0.5346 - val_loss: 0.7052 - val_acc: 0.4567\n",
      "Epoch 101/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.7008 - acc: 0.5356 - val_loss: 0.7318 - val_acc: 0.4921\n",
      "Epoch 102/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.7032 - acc: 0.5128 - val_loss: 0.7158 - val_acc: 0.4843\n",
      "Epoch 103/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7012 - acc: 0.5049 - val_loss: 0.7366 - val_acc: 0.5079\n",
      "Epoch 104/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7045 - acc: 0.5069 - val_loss: 0.7064 - val_acc: 0.4764\n",
      "Epoch 105/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6956 - acc: 0.5296 - val_loss: 0.7061 - val_acc: 0.4724\n",
      "Epoch 106/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6955 - acc: 0.5148 - val_loss: 0.7333 - val_acc: 0.5039\n",
      "Epoch 107/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.7011 - acc: 0.5217 - val_loss: 0.7134 - val_acc: 0.5118\n",
      "Epoch 108/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6999 - acc: 0.4921 - val_loss: 0.7447 - val_acc: 0.5000\n",
      "Epoch 109/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6956 - acc: 0.5277 - val_loss: 0.7262 - val_acc: 0.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6979 - acc: 0.5188 - val_loss: 0.7039 - val_acc: 0.4646\n",
      "Epoch 111/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6901 - acc: 0.5464 - val_loss: 0.7052 - val_acc: 0.4685\n",
      "Epoch 112/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6947 - acc: 0.5316 - val_loss: 0.7273 - val_acc: 0.5157\n",
      "Epoch 113/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6911 - acc: 0.5346 - val_loss: 0.7042 - val_acc: 0.4724\n",
      "Epoch 114/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6921 - acc: 0.5296 - val_loss: 0.7157 - val_acc: 0.5157\n",
      "Epoch 115/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6932 - acc: 0.5524 - val_loss: 0.7260 - val_acc: 0.4961\n",
      "Epoch 116/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6959 - acc: 0.5415 - val_loss: 0.7027 - val_acc: 0.4567\n",
      "Epoch 117/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7021 - acc: 0.5109 - val_loss: 0.7025 - val_acc: 0.5039\n",
      "Epoch 118/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6944 - acc: 0.5366 - val_loss: 0.7345 - val_acc: 0.5000\n",
      "Epoch 119/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6977 - acc: 0.5306 - val_loss: 0.7032 - val_acc: 0.4606\n",
      "Epoch 120/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.7003 - acc: 0.5089 - val_loss: 0.7077 - val_acc: 0.5079\n",
      "Epoch 121/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.7008 - acc: 0.5336 - val_loss: 0.7212 - val_acc: 0.4961\n",
      "Epoch 122/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6988 - acc: 0.5158 - val_loss: 0.7249 - val_acc: 0.4882\n",
      "Epoch 123/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7004 - acc: 0.5138 - val_loss: 0.7161 - val_acc: 0.5000\n",
      "Epoch 124/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6975 - acc: 0.5069 - val_loss: 0.7104 - val_acc: 0.5000\n",
      "Epoch 125/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6939 - acc: 0.5316 - val_loss: 0.7021 - val_acc: 0.4685\n",
      "Epoch 126/10000\n",
      "1012/1012 [==============================] - 0s 37us/step - loss: 0.6969 - acc: 0.5346 - val_loss: 0.7045 - val_acc: 0.4961\n",
      "Epoch 127/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6948 - acc: 0.5346 - val_loss: 0.7351 - val_acc: 0.5039\n",
      "Epoch 128/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6966 - acc: 0.5316 - val_loss: 0.7371 - val_acc: 0.5000\n",
      "Epoch 129/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6951 - acc: 0.5435 - val_loss: 0.7043 - val_acc: 0.5079\n",
      "Epoch 130/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7020 - acc: 0.5079 - val_loss: 0.7275 - val_acc: 0.4961\n",
      "Epoch 131/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6947 - acc: 0.5148 - val_loss: 0.7027 - val_acc: 0.4567\n",
      "Epoch 132/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7002 - acc: 0.5109 - val_loss: 0.7142 - val_acc: 0.5118\n",
      "Epoch 133/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6933 - acc: 0.5326 - val_loss: 0.7531 - val_acc: 0.5039\n",
      "Epoch 134/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7046 - acc: 0.5030 - val_loss: 0.7221 - val_acc: 0.4961\n",
      "Epoch 135/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6967 - acc: 0.5326 - val_loss: 0.7067 - val_acc: 0.5118\n",
      "Epoch 136/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.7006 - acc: 0.4960 - val_loss: 0.7073 - val_acc: 0.5197\n",
      "Epoch 137/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6993 - acc: 0.5040 - val_loss: 0.7080 - val_acc: 0.5000\n",
      "Epoch 138/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.7015 - acc: 0.5089 - val_loss: 0.7080 - val_acc: 0.4921\n",
      "Epoch 139/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6979 - acc: 0.5208 - val_loss: 0.7141 - val_acc: 0.5118\n",
      "Epoch 140/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6892 - acc: 0.5435 - val_loss: 0.7029 - val_acc: 0.4646\n",
      "Epoch 141/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6898 - acc: 0.5425 - val_loss: 0.7721 - val_acc: 0.4961\n",
      "Epoch 142/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.7019 - acc: 0.5375 - val_loss: 0.7023 - val_acc: 0.4528\n",
      "Epoch 143/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6900 - acc: 0.5326 - val_loss: 0.7080 - val_acc: 0.5118\n",
      "Epoch 144/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6966 - acc: 0.5237 - val_loss: 0.7071 - val_acc: 0.5039\n",
      "Epoch 145/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6937 - acc: 0.5217 - val_loss: 0.7529 - val_acc: 0.4961\n",
      "Epoch 146/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6944 - acc: 0.5247 - val_loss: 0.7199 - val_acc: 0.5000\n",
      "Epoch 147/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6911 - acc: 0.5316 - val_loss: 0.7189 - val_acc: 0.5118\n",
      "Epoch 148/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6967 - acc: 0.5227 - val_loss: 0.7656 - val_acc: 0.4961\n",
      "Epoch 149/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6952 - acc: 0.5385 - val_loss: 0.7363 - val_acc: 0.5079\n",
      "Epoch 150/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6952 - acc: 0.5287 - val_loss: 0.7072 - val_acc: 0.4961\n",
      "Epoch 151/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6913 - acc: 0.5326 - val_loss: 0.7201 - val_acc: 0.5118\n",
      "Epoch 152/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6908 - acc: 0.5484 - val_loss: 0.7320 - val_acc: 0.5079\n",
      "Epoch 153/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6929 - acc: 0.5326 - val_loss: 0.7118 - val_acc: 0.4882\n",
      "Epoch 154/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6991 - acc: 0.4960 - val_loss: 0.7261 - val_acc: 0.5039\n",
      "Epoch 155/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6897 - acc: 0.5316 - val_loss: 0.7060 - val_acc: 0.4882\n",
      "Epoch 156/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6969 - acc: 0.4990 - val_loss: 0.7063 - val_acc: 0.4921\n",
      "Epoch 157/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6960 - acc: 0.5346 - val_loss: 0.7045 - val_acc: 0.4567\n",
      "Epoch 158/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6936 - acc: 0.5208 - val_loss: 0.7128 - val_acc: 0.5039\n",
      "Epoch 159/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6945 - acc: 0.5198 - val_loss: 0.7112 - val_acc: 0.4961\n",
      "Epoch 160/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6910 - acc: 0.5316 - val_loss: 0.7030 - val_acc: 0.4961\n",
      "Epoch 161/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6928 - acc: 0.5158 - val_loss: 0.7080 - val_acc: 0.5000\n",
      "Epoch 162/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6983 - acc: 0.5287 - val_loss: 0.7034 - val_acc: 0.4882\n",
      "Epoch 163/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6920 - acc: 0.5277 - val_loss: 0.7119 - val_acc: 0.5000\n",
      "Epoch 164/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6959 - acc: 0.5257 - val_loss: 0.7009 - val_acc: 0.4961\n",
      "Epoch 165/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6926 - acc: 0.5148 - val_loss: 0.7084 - val_acc: 0.5039\n",
      "Epoch 166/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6924 - acc: 0.5168 - val_loss: 0.7012 - val_acc: 0.4843\n",
      "Epoch 167/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6899 - acc: 0.5455 - val_loss: 0.7116 - val_acc: 0.5118\n",
      "Epoch 168/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6960 - acc: 0.5208 - val_loss: 0.7009 - val_acc: 0.4764\n",
      "Epoch 169/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6920 - acc: 0.5316 - val_loss: 0.7083 - val_acc: 0.4961\n",
      "Epoch 170/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6955 - acc: 0.5119 - val_loss: 0.7213 - val_acc: 0.4882\n",
      "Epoch 171/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6916 - acc: 0.5356 - val_loss: 0.7001 - val_acc: 0.4843\n",
      "Epoch 172/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6931 - acc: 0.5287 - val_loss: 0.7169 - val_acc: 0.5039\n",
      "Epoch 173/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6961 - acc: 0.5287 - val_loss: 0.7048 - val_acc: 0.4921\n",
      "Epoch 174/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6945 - acc: 0.5257 - val_loss: 0.7118 - val_acc: 0.5118\n",
      "Epoch 175/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6966 - acc: 0.5109 - val_loss: 0.7140 - val_acc: 0.5000\n",
      "Epoch 176/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6978 - acc: 0.5099 - val_loss: 0.7132 - val_acc: 0.5079\n",
      "Epoch 177/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6933 - acc: 0.5208 - val_loss: 0.7015 - val_acc: 0.4646\n",
      "Epoch 178/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6962 - acc: 0.5306 - val_loss: 0.7020 - val_acc: 0.4724\n",
      "Epoch 179/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6907 - acc: 0.5217 - val_loss: 0.7150 - val_acc: 0.4921\n",
      "Epoch 180/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6947 - acc: 0.5227 - val_loss: 0.7185 - val_acc: 0.5000\n",
      "Epoch 181/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6975 - acc: 0.5198 - val_loss: 0.7007 - val_acc: 0.4803\n",
      "Epoch 182/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6900 - acc: 0.5425 - val_loss: 0.7240 - val_acc: 0.5039\n",
      "Epoch 183/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6959 - acc: 0.5198 - val_loss: 0.7145 - val_acc: 0.5079\n",
      "Epoch 184/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6929 - acc: 0.5158 - val_loss: 0.7147 - val_acc: 0.4961\n",
      "Epoch 185/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6945 - acc: 0.5227 - val_loss: 0.7174 - val_acc: 0.5039\n",
      "Epoch 186/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6928 - acc: 0.5277 - val_loss: 0.7095 - val_acc: 0.4961\n",
      "Epoch 187/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6939 - acc: 0.5296 - val_loss: 0.7256 - val_acc: 0.5000\n",
      "Epoch 188/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6952 - acc: 0.5168 - val_loss: 0.7022 - val_acc: 0.4724\n",
      "Epoch 189/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6904 - acc: 0.5336 - val_loss: 0.7013 - val_acc: 0.4646\n",
      "Epoch 190/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6865 - acc: 0.5366 - val_loss: 0.7280 - val_acc: 0.4961\n",
      "Epoch 191/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6937 - acc: 0.5217 - val_loss: 0.7071 - val_acc: 0.5039\n",
      "Epoch 192/10000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6953 - acc: 0.5020 - val_loss: 0.7053 - val_acc: 0.5000\n",
      "Epoch 193/10000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6917 - acc: 0.5247 - val_loss: 0.7019 - val_acc: 0.4606\n",
      "Epoch 194/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6868 - acc: 0.5405 - val_loss: 0.7088 - val_acc: 0.4961\n",
      "Epoch 195/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6958 - acc: 0.5316 - val_loss: 0.7119 - val_acc: 0.5079\n",
      "Epoch 196/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6918 - acc: 0.5119 - val_loss: 0.7488 - val_acc: 0.5039\n",
      "Epoch 197/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.7009 - acc: 0.5217 - val_loss: 0.7113 - val_acc: 0.5118\n",
      "Epoch 198/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6934 - acc: 0.5237 - val_loss: 0.7031 - val_acc: 0.4882\n",
      "Epoch 199/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6890 - acc: 0.5553 - val_loss: 0.7638 - val_acc: 0.4961\n",
      "Epoch 200/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6992 - acc: 0.5316 - val_loss: 0.7475 - val_acc: 0.4961\n",
      "Epoch 201/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6939 - acc: 0.5296 - val_loss: 0.7065 - val_acc: 0.4882\n",
      "Epoch 202/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6903 - acc: 0.5336 - val_loss: 0.7100 - val_acc: 0.4843\n",
      "Epoch 203/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6907 - acc: 0.5208 - val_loss: 0.7326 - val_acc: 0.5039\n",
      "Epoch 204/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6924 - acc: 0.5425 - val_loss: 0.7177 - val_acc: 0.5039\n",
      "Epoch 205/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6969 - acc: 0.5128 - val_loss: 0.7121 - val_acc: 0.5079\n",
      "Epoch 206/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6905 - acc: 0.5395 - val_loss: 0.7518 - val_acc: 0.4961\n",
      "Epoch 207/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6940 - acc: 0.5415 - val_loss: 0.7043 - val_acc: 0.4528\n",
      "Epoch 208/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6940 - acc: 0.5356 - val_loss: 0.7361 - val_acc: 0.5039\n",
      "Epoch 209/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6972 - acc: 0.5217 - val_loss: 0.7079 - val_acc: 0.4961\n",
      "Epoch 210/10000\n",
      "1012/1012 [==============================] - 0s 20us/step - loss: 0.6888 - acc: 0.5267 - val_loss: 0.7069 - val_acc: 0.4803\n",
      "Epoch 211/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6904 - acc: 0.5375 - val_loss: 0.7173 - val_acc: 0.5000\n",
      "Epoch 212/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6980 - acc: 0.5000 - val_loss: 0.7169 - val_acc: 0.5000\n",
      "Epoch 213/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6891 - acc: 0.5287 - val_loss: 0.7258 - val_acc: 0.5039\n",
      "Epoch 214/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6928 - acc: 0.5346 - val_loss: 0.7210 - val_acc: 0.4961\n",
      "Epoch 215/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6949 - acc: 0.5267 - val_loss: 0.7111 - val_acc: 0.5000\n",
      "Epoch 216/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6889 - acc: 0.5287 - val_loss: 0.7075 - val_acc: 0.4646\n",
      "Epoch 217/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6897 - acc: 0.5296 - val_loss: 0.7556 - val_acc: 0.4961\n",
      "Epoch 218/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6958 - acc: 0.5306 - val_loss: 0.7496 - val_acc: 0.4961\n",
      "Epoch 219/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6976 - acc: 0.5267 - val_loss: 0.7154 - val_acc: 0.5039\n",
      "Epoch 220/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6904 - acc: 0.5366 - val_loss: 0.7082 - val_acc: 0.4921\n",
      "Epoch 221/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6965 - acc: 0.5069 - val_loss: 0.7379 - val_acc: 0.5079\n",
      "Epoch 222/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6893 - acc: 0.5603 - val_loss: 0.7043 - val_acc: 0.4724\n",
      "Epoch 223/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6872 - acc: 0.5455 - val_loss: 0.7128 - val_acc: 0.4921\n",
      "Epoch 224/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6935 - acc: 0.5257 - val_loss: 0.7328 - val_acc: 0.5039\n",
      "Epoch 225/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6969 - acc: 0.5119 - val_loss: 0.7081 - val_acc: 0.4882\n",
      "Epoch 226/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6890 - acc: 0.5257 - val_loss: 0.7142 - val_acc: 0.5079\n",
      "Epoch 227/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6934 - acc: 0.5484 - val_loss: 0.7692 - val_acc: 0.5000\n",
      "Epoch 228/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6983 - acc: 0.5198 - val_loss: 0.7058 - val_acc: 0.4882\n",
      "Epoch 229/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6926 - acc: 0.5267 - val_loss: 0.7037 - val_acc: 0.5079\n",
      "Epoch 230/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6920 - acc: 0.5405 - val_loss: 0.7048 - val_acc: 0.4803\n",
      "Epoch 231/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6891 - acc: 0.5395 - val_loss: 0.7265 - val_acc: 0.5039\n",
      "Epoch 232/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6982 - acc: 0.5178 - val_loss: 0.7152 - val_acc: 0.5000\n",
      "Epoch 233/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6972 - acc: 0.4980 - val_loss: 0.7092 - val_acc: 0.4961\n",
      "Epoch 234/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6916 - acc: 0.5247 - val_loss: 0.7164 - val_acc: 0.5000\n",
      "Epoch 235/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6917 - acc: 0.5425 - val_loss: 0.7236 - val_acc: 0.4921\n",
      "Epoch 236/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6912 - acc: 0.5326 - val_loss: 0.7039 - val_acc: 0.4646\n",
      "Epoch 237/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6917 - acc: 0.5326 - val_loss: 0.7416 - val_acc: 0.5000\n",
      "Epoch 238/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6939 - acc: 0.5464 - val_loss: 0.7184 - val_acc: 0.4961\n",
      "Epoch 239/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6935 - acc: 0.5455 - val_loss: 0.7095 - val_acc: 0.4961\n",
      "Epoch 240/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6964 - acc: 0.5198 - val_loss: 0.7289 - val_acc: 0.5039\n",
      "Epoch 241/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6913 - acc: 0.5474 - val_loss: 0.7025 - val_acc: 0.4685\n",
      "Epoch 242/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6869 - acc: 0.5543 - val_loss: 0.7052 - val_acc: 0.4843\n",
      "Epoch 243/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6940 - acc: 0.5178 - val_loss: 0.7287 - val_acc: 0.5079\n",
      "Epoch 244/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6905 - acc: 0.5336 - val_loss: 0.7314 - val_acc: 0.5000\n",
      "Epoch 245/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6903 - acc: 0.5405 - val_loss: 0.7026 - val_acc: 0.4803\n",
      "Epoch 246/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6852 - acc: 0.5445 - val_loss: 0.7026 - val_acc: 0.4488\n",
      "Epoch 247/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6887 - acc: 0.5375 - val_loss: 0.7538 - val_acc: 0.4961\n",
      "Epoch 248/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6985 - acc: 0.5227 - val_loss: 0.7151 - val_acc: 0.5039\n",
      "Epoch 249/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6940 - acc: 0.5287 - val_loss: 0.7027 - val_acc: 0.4370\n",
      "Epoch 250/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6924 - acc: 0.5237 - val_loss: 0.7125 - val_acc: 0.5079\n",
      "Epoch 251/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6998 - acc: 0.5079 - val_loss: 0.7030 - val_acc: 0.4646\n",
      "Epoch 252/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6920 - acc: 0.5099 - val_loss: 0.7090 - val_acc: 0.5000\n",
      "Epoch 253/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6924 - acc: 0.5227 - val_loss: 0.7293 - val_acc: 0.5039\n",
      "Epoch 254/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6984 - acc: 0.5069 - val_loss: 0.7054 - val_acc: 0.4764\n",
      "Epoch 255/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6889 - acc: 0.5435 - val_loss: 0.7112 - val_acc: 0.4961\n",
      "Epoch 256/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.7003 - acc: 0.5119 - val_loss: 0.7318 - val_acc: 0.5000\n",
      "Epoch 257/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6961 - acc: 0.5217 - val_loss: 0.7175 - val_acc: 0.5118\n",
      "Epoch 258/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6980 - acc: 0.5227 - val_loss: 0.7182 - val_acc: 0.5079\n",
      "Epoch 259/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6921 - acc: 0.5366 - val_loss: 0.7061 - val_acc: 0.4843\n",
      "Epoch 260/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6930 - acc: 0.5415 - val_loss: 0.7037 - val_acc: 0.4488\n",
      "Epoch 261/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6896 - acc: 0.5524 - val_loss: 0.7289 - val_acc: 0.4961\n",
      "Epoch 262/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6902 - acc: 0.5257 - val_loss: 0.7169 - val_acc: 0.5039\n",
      "Epoch 263/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6906 - acc: 0.5375 - val_loss: 0.7346 - val_acc: 0.5000\n",
      "Epoch 264/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6932 - acc: 0.5336 - val_loss: 0.7094 - val_acc: 0.5118\n",
      "Epoch 265/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6908 - acc: 0.5217 - val_loss: 0.7209 - val_acc: 0.4961\n",
      "Epoch 266/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6892 - acc: 0.5474 - val_loss: 0.7137 - val_acc: 0.5079\n",
      "Epoch 267/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6921 - acc: 0.5208 - val_loss: 0.7080 - val_acc: 0.4882\n",
      "Epoch 268/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6897 - acc: 0.5346 - val_loss: 0.7035 - val_acc: 0.4606\n",
      "Epoch 269/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6956 - acc: 0.5464 - val_loss: 0.7036 - val_acc: 0.4449\n",
      "Epoch 270/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6875 - acc: 0.5524 - val_loss: 0.7031 - val_acc: 0.4764\n",
      "Epoch 271/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6901 - acc: 0.5445 - val_loss: 0.7096 - val_acc: 0.4882\n",
      "Epoch 272/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6934 - acc: 0.5198 - val_loss: 0.7040 - val_acc: 0.4449\n",
      "Epoch 273/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6888 - acc: 0.5455 - val_loss: 0.7240 - val_acc: 0.5000\n",
      "Epoch 274/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6940 - acc: 0.5326 - val_loss: 0.7110 - val_acc: 0.4843\n",
      "Epoch 275/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6914 - acc: 0.5366 - val_loss: 0.7084 - val_acc: 0.5039\n",
      "Epoch 276/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6936 - acc: 0.5198 - val_loss: 0.7033 - val_acc: 0.4803\n",
      "Epoch 277/10000\n",
      "1012/1012 [==============================] - 0s 20us/step - loss: 0.6921 - acc: 0.5356 - val_loss: 0.7036 - val_acc: 0.4409\n",
      "Epoch 278/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6914 - acc: 0.5208 - val_loss: 0.7098 - val_acc: 0.4921\n",
      "Epoch 279/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6921 - acc: 0.5375 - val_loss: 0.7107 - val_acc: 0.4843\n",
      "Epoch 280/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6920 - acc: 0.5375 - val_loss: 0.7031 - val_acc: 0.4606\n",
      "Epoch 281/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6860 - acc: 0.5474 - val_loss: 0.7122 - val_acc: 0.5079\n",
      "Epoch 282/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6912 - acc: 0.5267 - val_loss: 0.7161 - val_acc: 0.5079\n",
      "Epoch 283/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6913 - acc: 0.5435 - val_loss: 0.7082 - val_acc: 0.4921\n",
      "Epoch 284/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6947 - acc: 0.5178 - val_loss: 0.7225 - val_acc: 0.4961\n",
      "Epoch 285/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6929 - acc: 0.5474 - val_loss: 0.7288 - val_acc: 0.5039\n",
      "Epoch 286/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6894 - acc: 0.5296 - val_loss: 0.7015 - val_acc: 0.4567\n",
      "Epoch 287/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6916 - acc: 0.5208 - val_loss: 0.7033 - val_acc: 0.4843\n",
      "Epoch 288/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6887 - acc: 0.5474 - val_loss: 0.7693 - val_acc: 0.5039\n",
      "Epoch 289/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6959 - acc: 0.5346 - val_loss: 0.7262 - val_acc: 0.5039\n",
      "Epoch 290/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6945 - acc: 0.5296 - val_loss: 0.7012 - val_acc: 0.5000\n",
      "Epoch 291/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6953 - acc: 0.5148 - val_loss: 0.7892 - val_acc: 0.4961\n",
      "Epoch 292/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.7014 - acc: 0.5109 - val_loss: 0.7230 - val_acc: 0.5000\n",
      "Epoch 293/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6929 - acc: 0.5128 - val_loss: 0.7007 - val_acc: 0.4724\n",
      "Epoch 294/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6920 - acc: 0.5237 - val_loss: 0.7075 - val_acc: 0.5039\n",
      "Epoch 295/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6916 - acc: 0.5198 - val_loss: 0.7003 - val_acc: 0.4764\n",
      "Epoch 296/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6937 - acc: 0.5326 - val_loss: 0.7057 - val_acc: 0.4961\n",
      "Epoch 297/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6888 - acc: 0.5277 - val_loss: 0.7154 - val_acc: 0.5000\n",
      "Epoch 298/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6922 - acc: 0.5267 - val_loss: 0.7012 - val_acc: 0.4606\n",
      "Epoch 299/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6901 - acc: 0.5336 - val_loss: 0.7000 - val_acc: 0.4685\n",
      "Epoch 300/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6895 - acc: 0.5267 - val_loss: 0.7549 - val_acc: 0.5000\n",
      "Epoch 301/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6943 - acc: 0.5356 - val_loss: 0.7333 - val_acc: 0.4961\n",
      "Epoch 302/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6964 - acc: 0.5277 - val_loss: 0.7026 - val_acc: 0.4961\n",
      "Epoch 303/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6922 - acc: 0.5287 - val_loss: 0.6983 - val_acc: 0.4921\n",
      "Epoch 304/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6904 - acc: 0.5208 - val_loss: 0.7064 - val_acc: 0.5157\n",
      "Epoch 305/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6900 - acc: 0.5405 - val_loss: 0.7141 - val_acc: 0.4961\n",
      "Epoch 306/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6922 - acc: 0.5306 - val_loss: 0.7179 - val_acc: 0.4961\n",
      "Epoch 307/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6914 - acc: 0.5217 - val_loss: 0.7454 - val_acc: 0.4961\n",
      "Epoch 308/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6949 - acc: 0.5257 - val_loss: 0.7015 - val_acc: 0.4803\n",
      "Epoch 309/10000\n",
      "1012/1012 [==============================] - 0s 21us/step - loss: 0.6919 - acc: 0.5366 - val_loss: 0.7522 - val_acc: 0.5039\n",
      "Epoch 310/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6980 - acc: 0.5148 - val_loss: 0.7074 - val_acc: 0.5079\n",
      "Epoch 311/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6915 - acc: 0.5267 - val_loss: 0.7242 - val_acc: 0.5079\n",
      "Epoch 312/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6902 - acc: 0.5455 - val_loss: 0.7029 - val_acc: 0.4843\n",
      "Epoch 313/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6891 - acc: 0.5356 - val_loss: 0.7172 - val_acc: 0.4961\n",
      "Epoch 314/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6951 - acc: 0.5089 - val_loss: 0.7124 - val_acc: 0.4961\n",
      "Epoch 315/10000\n",
      "1012/1012 [==============================] - 0s 35us/step - loss: 0.6935 - acc: 0.5198 - val_loss: 0.7233 - val_acc: 0.5000\n",
      "Epoch 316/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6941 - acc: 0.5267 - val_loss: 0.7131 - val_acc: 0.5079\n",
      "Epoch 317/10000\n",
      "1012/1012 [==============================] - 0s 35us/step - loss: 0.6930 - acc: 0.5128 - val_loss: 0.7375 - val_acc: 0.5000\n",
      "Epoch 318/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6950 - acc: 0.5227 - val_loss: 0.7081 - val_acc: 0.4882\n",
      "Epoch 319/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6929 - acc: 0.5306 - val_loss: 0.7040 - val_acc: 0.4882\n",
      "Epoch 320/10000\n",
      "1012/1012 [==============================] - 0s 45us/step - loss: 0.6906 - acc: 0.5168 - val_loss: 0.7113 - val_acc: 0.5079\n",
      "Epoch 321/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6925 - acc: 0.5247 - val_loss: 0.7087 - val_acc: 0.4961\n",
      "Epoch 322/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6874 - acc: 0.5504 - val_loss: 0.7056 - val_acc: 0.4843\n",
      "Epoch 323/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6907 - acc: 0.5346 - val_loss: 0.7347 - val_acc: 0.5039\n",
      "Epoch 324/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6918 - acc: 0.5425 - val_loss: 0.7070 - val_acc: 0.4882\n",
      "Epoch 325/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6947 - acc: 0.5346 - val_loss: 0.7388 - val_acc: 0.5000\n",
      "Epoch 326/10000\n",
      "1012/1012 [==============================] - 0s 35us/step - loss: 0.6953 - acc: 0.5306 - val_loss: 0.7042 - val_acc: 0.4685\n",
      "Epoch 327/10000\n",
      "1012/1012 [==============================] - 0s 23us/step - loss: 0.6889 - acc: 0.5425 - val_loss: 0.7473 - val_acc: 0.4961\n",
      "Epoch 328/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6977 - acc: 0.5178 - val_loss: 0.7074 - val_acc: 0.4961\n",
      "Epoch 329/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6924 - acc: 0.5287 - val_loss: 0.7271 - val_acc: 0.5079\n",
      "Epoch 330/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6936 - acc: 0.5257 - val_loss: 0.7091 - val_acc: 0.4843\n",
      "Epoch 331/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6873 - acc: 0.5326 - val_loss: 0.7177 - val_acc: 0.4921\n",
      "Epoch 332/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6943 - acc: 0.5464 - val_loss: 0.7038 - val_acc: 0.4803\n",
      "Epoch 333/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6890 - acc: 0.5385 - val_loss: 0.7160 - val_acc: 0.4921\n",
      "Epoch 334/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6896 - acc: 0.5277 - val_loss: 0.7033 - val_acc: 0.4724\n",
      "Epoch 335/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6939 - acc: 0.5247 - val_loss: 0.7362 - val_acc: 0.5118\n",
      "Epoch 336/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6950 - acc: 0.5059 - val_loss: 0.7290 - val_acc: 0.5000\n",
      "Epoch 337/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6918 - acc: 0.5385 - val_loss: 0.7044 - val_acc: 0.4724\n",
      "Epoch 338/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6902 - acc: 0.5287 - val_loss: 0.7530 - val_acc: 0.4961\n",
      "Epoch 339/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6974 - acc: 0.5247 - val_loss: 0.7128 - val_acc: 0.4961\n",
      "Epoch 340/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6890 - acc: 0.5395 - val_loss: 0.7034 - val_acc: 0.4961\n",
      "Epoch 341/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6890 - acc: 0.5287 - val_loss: 0.7039 - val_acc: 0.4961\n",
      "Epoch 342/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6922 - acc: 0.5306 - val_loss: 0.7147 - val_acc: 0.5039\n",
      "Epoch 343/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6874 - acc: 0.5484 - val_loss: 0.7020 - val_acc: 0.4724\n",
      "Epoch 344/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6925 - acc: 0.5366 - val_loss: 0.7107 - val_acc: 0.4882\n",
      "Epoch 345/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6930 - acc: 0.5277 - val_loss: 0.7080 - val_acc: 0.5118\n",
      "Epoch 346/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6910 - acc: 0.5366 - val_loss: 0.7392 - val_acc: 0.5000\n",
      "Epoch 347/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6945 - acc: 0.5138 - val_loss: 0.7136 - val_acc: 0.5079\n",
      "Epoch 348/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6875 - acc: 0.5356 - val_loss: 0.7292 - val_acc: 0.5000\n",
      "Epoch 349/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6927 - acc: 0.5385 - val_loss: 0.7046 - val_acc: 0.4921\n",
      "Epoch 350/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6878 - acc: 0.5455 - val_loss: 0.7063 - val_acc: 0.4843\n",
      "Epoch 351/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6964 - acc: 0.5237 - val_loss: 0.7132 - val_acc: 0.5118\n",
      "Epoch 352/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6867 - acc: 0.5425 - val_loss: 0.7495 - val_acc: 0.5039\n",
      "Epoch 353/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6943 - acc: 0.5119 - val_loss: 0.7043 - val_acc: 0.4882\n",
      "Epoch 354/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6870 - acc: 0.5336 - val_loss: 0.7132 - val_acc: 0.5039\n",
      "Epoch 355/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6930 - acc: 0.5188 - val_loss: 0.7122 - val_acc: 0.5079\n",
      "Epoch 356/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6908 - acc: 0.5168 - val_loss: 0.7180 - val_acc: 0.4921\n",
      "Epoch 357/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6911 - acc: 0.5217 - val_loss: 0.7129 - val_acc: 0.5079\n",
      "Epoch 358/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6918 - acc: 0.5464 - val_loss: 0.7060 - val_acc: 0.4882\n",
      "Epoch 359/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6883 - acc: 0.5543 - val_loss: 0.7324 - val_acc: 0.5000\n",
      "Epoch 360/10000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6977 - acc: 0.5128 - val_loss: 0.7177 - val_acc: 0.4961\n",
      "Epoch 361/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6951 - acc: 0.5227 - val_loss: 0.7072 - val_acc: 0.4921\n",
      "Epoch 362/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6905 - acc: 0.5326 - val_loss: 0.7041 - val_acc: 0.4882\n",
      "Epoch 363/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6900 - acc: 0.5375 - val_loss: 0.7088 - val_acc: 0.4843\n",
      "Epoch 364/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6895 - acc: 0.5445 - val_loss: 0.7192 - val_acc: 0.5039\n",
      "Epoch 365/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6905 - acc: 0.5247 - val_loss: 0.7023 - val_acc: 0.4803\n",
      "Epoch 366/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6911 - acc: 0.5415 - val_loss: 0.7148 - val_acc: 0.4961\n",
      "Epoch 367/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6851 - acc: 0.5504 - val_loss: 0.7185 - val_acc: 0.5039\n",
      "Epoch 368/10000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6937 - acc: 0.5494 - val_loss: 0.7155 - val_acc: 0.5118\n",
      "Epoch 369/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6921 - acc: 0.5296 - val_loss: 0.7023 - val_acc: 0.4724\n",
      "Epoch 370/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6908 - acc: 0.5287 - val_loss: 0.7043 - val_acc: 0.4882\n",
      "Epoch 371/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6864 - acc: 0.5375 - val_loss: 0.7192 - val_acc: 0.5039\n",
      "Epoch 372/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6865 - acc: 0.5425 - val_loss: 0.7428 - val_acc: 0.5039\n",
      "Epoch 373/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6909 - acc: 0.5336 - val_loss: 0.7271 - val_acc: 0.5079\n",
      "Epoch 374/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6965 - acc: 0.5128 - val_loss: 0.7093 - val_acc: 0.5157\n",
      "Epoch 375/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6914 - acc: 0.5336 - val_loss: 0.7072 - val_acc: 0.5039\n",
      "Epoch 376/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6921 - acc: 0.5287 - val_loss: 0.7208 - val_acc: 0.5000\n",
      "Epoch 377/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6909 - acc: 0.5247 - val_loss: 0.7023 - val_acc: 0.4882\n",
      "Epoch 378/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6932 - acc: 0.5109 - val_loss: 0.7237 - val_acc: 0.5000\n",
      "Epoch 379/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6928 - acc: 0.5069 - val_loss: 0.7114 - val_acc: 0.5157\n",
      "Epoch 380/10000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6857 - acc: 0.5494 - val_loss: 0.7174 - val_acc: 0.5079\n",
      "Epoch 381/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6901 - acc: 0.5415 - val_loss: 0.7010 - val_acc: 0.4764\n",
      "Epoch 382/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6908 - acc: 0.5366 - val_loss: 0.7047 - val_acc: 0.4803\n",
      "Epoch 383/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6907 - acc: 0.5356 - val_loss: 0.7275 - val_acc: 0.5000\n",
      "Epoch 384/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6927 - acc: 0.5287 - val_loss: 0.7099 - val_acc: 0.5157\n",
      "Epoch 385/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6920 - acc: 0.5395 - val_loss: 0.7404 - val_acc: 0.5000\n",
      "Epoch 386/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6980 - acc: 0.5188 - val_loss: 0.7010 - val_acc: 0.4724\n",
      "Epoch 387/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6892 - acc: 0.5375 - val_loss: 0.7073 - val_acc: 0.4921\n",
      "Epoch 388/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6916 - acc: 0.5316 - val_loss: 0.7072 - val_acc: 0.5157\n",
      "Epoch 389/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6887 - acc: 0.5484 - val_loss: 0.7107 - val_acc: 0.5276\n",
      "Epoch 390/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6861 - acc: 0.5415 - val_loss: 0.7340 - val_acc: 0.5000\n",
      "Epoch 391/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6927 - acc: 0.5366 - val_loss: 0.7188 - val_acc: 0.5039\n",
      "Epoch 392/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6916 - acc: 0.5217 - val_loss: 0.7272 - val_acc: 0.4921\n",
      "Epoch 393/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6927 - acc: 0.5326 - val_loss: 0.7101 - val_acc: 0.5079\n",
      "Epoch 394/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6920 - acc: 0.5405 - val_loss: 0.7039 - val_acc: 0.4961\n",
      "Epoch 395/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6925 - acc: 0.5267 - val_loss: 0.7254 - val_acc: 0.5000\n",
      "Epoch 396/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6939 - acc: 0.5287 - val_loss: 0.7244 - val_acc: 0.5000\n",
      "Epoch 397/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6922 - acc: 0.5316 - val_loss: 0.7014 - val_acc: 0.4921\n",
      "Epoch 398/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6931 - acc: 0.5356 - val_loss: 0.7020 - val_acc: 0.5079\n",
      "Epoch 399/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6902 - acc: 0.5474 - val_loss: 0.7099 - val_acc: 0.5079\n",
      "Epoch 400/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6883 - acc: 0.5395 - val_loss: 0.7108 - val_acc: 0.5000\n",
      "Epoch 401/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6903 - acc: 0.5277 - val_loss: 0.7028 - val_acc: 0.4606\n",
      "Epoch 402/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6901 - acc: 0.5385 - val_loss: 0.7465 - val_acc: 0.5000\n",
      "Epoch 403/10000\n",
      "1012/1012 [==============================] - 0s 36us/step - loss: 0.6899 - acc: 0.5593 - val_loss: 0.7276 - val_acc: 0.5079\n",
      "Epoch 404/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6865 - acc: 0.5316 - val_loss: 0.7493 - val_acc: 0.4961\n",
      "Epoch 405/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6931 - acc: 0.5346 - val_loss: 0.7202 - val_acc: 0.4961\n",
      "Epoch 406/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6892 - acc: 0.5583 - val_loss: 0.7244 - val_acc: 0.5079\n",
      "Epoch 407/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6941 - acc: 0.5336 - val_loss: 0.7047 - val_acc: 0.4803\n",
      "Epoch 408/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6880 - acc: 0.5375 - val_loss: 0.7068 - val_acc: 0.4843\n",
      "Epoch 409/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6874 - acc: 0.5356 - val_loss: 0.7026 - val_acc: 0.4724\n",
      "Epoch 410/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6912 - acc: 0.5119 - val_loss: 0.7123 - val_acc: 0.4961\n",
      "Epoch 411/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6886 - acc: 0.5326 - val_loss: 0.7104 - val_acc: 0.4882\n",
      "Epoch 412/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6864 - acc: 0.5366 - val_loss: 0.7037 - val_acc: 0.4724\n",
      "Epoch 413/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6907 - acc: 0.5504 - val_loss: 0.7173 - val_acc: 0.5157\n",
      "Epoch 414/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6918 - acc: 0.5168 - val_loss: 0.7035 - val_acc: 0.4646\n",
      "Epoch 415/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6907 - acc: 0.5415 - val_loss: 0.7079 - val_acc: 0.4961\n",
      "Epoch 416/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6912 - acc: 0.5366 - val_loss: 0.7658 - val_acc: 0.5000\n",
      "Epoch 417/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6993 - acc: 0.5198 - val_loss: 0.7125 - val_acc: 0.5118\n",
      "Epoch 418/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6877 - acc: 0.5484 - val_loss: 0.7056 - val_acc: 0.4921\n",
      "Epoch 419/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6927 - acc: 0.5336 - val_loss: 0.7216 - val_acc: 0.5079\n",
      "Epoch 420/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6956 - acc: 0.5366 - val_loss: 0.7034 - val_acc: 0.4606\n",
      "Epoch 421/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6876 - acc: 0.5415 - val_loss: 0.7275 - val_acc: 0.5000\n",
      "Epoch 422/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6939 - acc: 0.5188 - val_loss: 0.7035 - val_acc: 0.4685\n",
      "Epoch 423/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6857 - acc: 0.5514 - val_loss: 0.7025 - val_acc: 0.4606\n",
      "Epoch 424/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6883 - acc: 0.5395 - val_loss: 0.7052 - val_acc: 0.4843\n",
      "Epoch 425/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6936 - acc: 0.5306 - val_loss: 0.7039 - val_acc: 0.4488\n",
      "Epoch 426/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6881 - acc: 0.5198 - val_loss: 0.7639 - val_acc: 0.4961\n",
      "Epoch 427/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6958 - acc: 0.5395 - val_loss: 0.7124 - val_acc: 0.4764\n",
      "Epoch 428/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6885 - acc: 0.5346 - val_loss: 0.7041 - val_acc: 0.4764\n",
      "Epoch 429/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6897 - acc: 0.5464 - val_loss: 0.7140 - val_acc: 0.4843\n",
      "Epoch 430/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6938 - acc: 0.5425 - val_loss: 0.7052 - val_acc: 0.4882\n",
      "Epoch 431/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6898 - acc: 0.5474 - val_loss: 0.7176 - val_acc: 0.5118\n",
      "Epoch 432/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6850 - acc: 0.5455 - val_loss: 0.7597 - val_acc: 0.4961\n",
      "Epoch 433/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6930 - acc: 0.5455 - val_loss: 0.7482 - val_acc: 0.5039\n",
      "Epoch 434/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6936 - acc: 0.5484 - val_loss: 0.7273 - val_acc: 0.5079\n",
      "Epoch 435/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6855 - acc: 0.5534 - val_loss: 0.7071 - val_acc: 0.4646\n",
      "Epoch 436/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6862 - acc: 0.5316 - val_loss: 0.7197 - val_acc: 0.5197\n",
      "Epoch 437/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6898 - acc: 0.5484 - val_loss: 0.7594 - val_acc: 0.5000\n",
      "Epoch 438/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6985 - acc: 0.5217 - val_loss: 0.7075 - val_acc: 0.4921\n",
      "Epoch 439/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6856 - acc: 0.5642 - val_loss: 0.7157 - val_acc: 0.4882\n",
      "Epoch 440/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6930 - acc: 0.5237 - val_loss: 0.7034 - val_acc: 0.4882\n",
      "Epoch 441/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6881 - acc: 0.5455 - val_loss: 0.7314 - val_acc: 0.5000\n",
      "Epoch 442/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6927 - acc: 0.5395 - val_loss: 0.7152 - val_acc: 0.4961\n",
      "Epoch 443/10000\n",
      "1012/1012 [==============================] - 0s 34us/step - loss: 0.6913 - acc: 0.5316 - val_loss: 0.7171 - val_acc: 0.5157\n",
      "Epoch 444/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6950 - acc: 0.5395 - val_loss: 0.7034 - val_acc: 0.4843\n",
      "Epoch 445/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6883 - acc: 0.5415 - val_loss: 0.7039 - val_acc: 0.4567\n",
      "Epoch 446/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6853 - acc: 0.5534 - val_loss: 0.7089 - val_acc: 0.4921\n",
      "Epoch 447/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6955 - acc: 0.5375 - val_loss: 0.7143 - val_acc: 0.5118\n",
      "Epoch 448/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6910 - acc: 0.5514 - val_loss: 0.7070 - val_acc: 0.4685\n",
      "Epoch 449/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6862 - acc: 0.5366 - val_loss: 0.7522 - val_acc: 0.5039\n",
      "Epoch 450/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6922 - acc: 0.5514 - val_loss: 0.7265 - val_acc: 0.5039\n",
      "Epoch 451/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6892 - acc: 0.5445 - val_loss: 0.7198 - val_acc: 0.5197\n",
      "Epoch 452/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6881 - acc: 0.5435 - val_loss: 0.7039 - val_acc: 0.4764\n",
      "Epoch 453/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6897 - acc: 0.5375 - val_loss: 0.7031 - val_acc: 0.4409\n",
      "Epoch 454/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6859 - acc: 0.5543 - val_loss: 0.7030 - val_acc: 0.4449\n",
      "Epoch 455/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6931 - acc: 0.5227 - val_loss: 0.7066 - val_acc: 0.5079\n",
      "Epoch 456/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6875 - acc: 0.5306 - val_loss: 0.7046 - val_acc: 0.4764\n",
      "Epoch 457/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6900 - acc: 0.5326 - val_loss: 0.7201 - val_acc: 0.5118\n",
      "Epoch 458/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6909 - acc: 0.5188 - val_loss: 0.7037 - val_acc: 0.4724\n",
      "Epoch 459/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6867 - acc: 0.5474 - val_loss: 0.7133 - val_acc: 0.5236\n",
      "Epoch 460/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6845 - acc: 0.5494 - val_loss: 0.7096 - val_acc: 0.5118\n",
      "Epoch 461/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6851 - acc: 0.5455 - val_loss: 0.7305 - val_acc: 0.5000\n",
      "Epoch 462/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6894 - acc: 0.5356 - val_loss: 0.7059 - val_acc: 0.5039\n",
      "Epoch 463/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6948 - acc: 0.5385 - val_loss: 0.7089 - val_acc: 0.5079\n",
      "Epoch 464/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6883 - acc: 0.5366 - val_loss: 0.7033 - val_acc: 0.4803\n",
      "Epoch 465/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6912 - acc: 0.5385 - val_loss: 0.7148 - val_acc: 0.5276\n",
      "Epoch 466/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6898 - acc: 0.5435 - val_loss: 0.7032 - val_acc: 0.4567\n",
      "Epoch 467/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6840 - acc: 0.5455 - val_loss: 0.7029 - val_acc: 0.4646\n",
      "Epoch 468/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6923 - acc: 0.5267 - val_loss: 0.7255 - val_acc: 0.5000\n",
      "Epoch 469/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6923 - acc: 0.5099 - val_loss: 0.7026 - val_acc: 0.4646\n",
      "Epoch 470/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6892 - acc: 0.5336 - val_loss: 0.7260 - val_acc: 0.5079\n",
      "Epoch 471/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6924 - acc: 0.5395 - val_loss: 0.7015 - val_acc: 0.4764\n",
      "Epoch 472/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6863 - acc: 0.5455 - val_loss: 0.7021 - val_acc: 0.4724\n",
      "Epoch 473/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6886 - acc: 0.5336 - val_loss: 0.7205 - val_acc: 0.5039\n",
      "Epoch 474/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6925 - acc: 0.5217 - val_loss: 0.7016 - val_acc: 0.4843\n",
      "Epoch 475/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6886 - acc: 0.5296 - val_loss: 0.7011 - val_acc: 0.4803\n",
      "Epoch 476/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6892 - acc: 0.5227 - val_loss: 0.7018 - val_acc: 0.4882\n",
      "Epoch 477/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6917 - acc: 0.5474 - val_loss: 0.7283 - val_acc: 0.4961\n",
      "Epoch 478/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6880 - acc: 0.5494 - val_loss: 0.7250 - val_acc: 0.5039\n",
      "Epoch 479/10000\n",
      "1012/1012 [==============================] - 0s 29us/step - loss: 0.6882 - acc: 0.5534 - val_loss: 0.7192 - val_acc: 0.5197\n",
      "Epoch 480/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6904 - acc: 0.5435 - val_loss: 0.7028 - val_acc: 0.4803\n",
      "Epoch 481/10000\n",
      "1012/1012 [==============================] - 0s 30us/step - loss: 0.6883 - acc: 0.5543 - val_loss: 0.7231 - val_acc: 0.5039\n",
      "Epoch 482/10000\n",
      "1012/1012 [==============================] - 0s 26us/step - loss: 0.6848 - acc: 0.5632 - val_loss: 0.7036 - val_acc: 0.5079\n",
      "Epoch 483/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6894 - acc: 0.5336 - val_loss: 0.7027 - val_acc: 0.4843\n",
      "Epoch 484/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6867 - acc: 0.5415 - val_loss: 0.7126 - val_acc: 0.5276\n",
      "Epoch 485/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6947 - acc: 0.5208 - val_loss: 0.7110 - val_acc: 0.5118\n",
      "Epoch 486/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6851 - acc: 0.5632 - val_loss: 0.7064 - val_acc: 0.4882\n",
      "Epoch 487/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6836 - acc: 0.5553 - val_loss: 0.7175 - val_acc: 0.5157\n",
      "Epoch 488/10000\n",
      "1012/1012 [==============================] - 0s 32us/step - loss: 0.6872 - acc: 0.5484 - val_loss: 0.7098 - val_acc: 0.5000\n",
      "Epoch 489/10000\n",
      "1012/1012 [==============================] - 0s 42us/step - loss: 0.6866 - acc: 0.5415 - val_loss: 0.7142 - val_acc: 0.5236\n",
      "Epoch 490/10000\n",
      "1012/1012 [==============================] - 0s 38us/step - loss: 0.6900 - acc: 0.5494 - val_loss: 0.7081 - val_acc: 0.5039\n",
      "Epoch 491/10000\n",
      "1012/1012 [==============================] - 0s 33us/step - loss: 0.6830 - acc: 0.5593 - val_loss: 0.7188 - val_acc: 0.5000\n",
      "Epoch 492/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6876 - acc: 0.5178 - val_loss: 0.7019 - val_acc: 0.5039\n",
      "Epoch 493/10000\n",
      "1012/1012 [==============================] - 0s 31us/step - loss: 0.6882 - acc: 0.5277 - val_loss: 0.7012 - val_acc: 0.5079\n",
      "Epoch 494/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6915 - acc: 0.5346 - val_loss: 0.7241 - val_acc: 0.5000\n",
      "Epoch 495/10000\n",
      "1012/1012 [==============================] - 0s 25us/step - loss: 0.6901 - acc: 0.5494 - val_loss: 0.7017 - val_acc: 0.4685\n",
      "Epoch 496/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6836 - acc: 0.5494 - val_loss: 0.7074 - val_acc: 0.4961\n",
      "Epoch 497/10000\n",
      "1012/1012 [==============================] - 0s 27us/step - loss: 0.6885 - acc: 0.5287 - val_loss: 0.7149 - val_acc: 0.5197\n",
      "Epoch 498/10000\n",
      "1012/1012 [==============================] - 0s 24us/step - loss: 0.6974 - acc: 0.5257 - val_loss: 0.7109 - val_acc: 0.5157\n",
      "Epoch 499/10000\n",
      "1012/1012 [==============================] - 0s 22us/step - loss: 0.6906 - acc: 0.5336 - val_loss: 0.7261 - val_acc: 0.5000\n",
      "Epoch 500/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6915 - acc: 0.5306 - val_loss: 0.7035 - val_acc: 0.4882\n",
      "Epoch 501/10000\n",
      "1012/1012 [==============================] - 0s 28us/step - loss: 0.6906 - acc: 0.5366 - val_loss: 0.7028 - val_acc: 0.5000\n",
      "Epoch 00501: early stopping\n",
      "Errors: 76  Correct :81\n",
      "Testing Accuracy: 51.59235668789809\n"
     ]
    }
   ],
   "source": [
    "La           = 2\n",
    "learningRate = 0.015\n",
    "L_Accuracy_Val   = []\n",
    "L_Accuracy_TR    = []\n",
    "L_Accuracy_Test  = []\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []\n",
    "W_Mat        = []\n",
    "\n",
    "if user_option_3 == \"1\" or user_option_3 == \"2\":\n",
    "    W_Now        = np.dot(100, W)\n",
    "    for i in range(0,400):\n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        if user_option_3 == \"1\":\n",
    "            Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "        elif user_option_3 == \"2\":\n",
    "            TrainingData1 = np.transpose(TrainingData)\n",
    "            Delta_E_D     = -np.dot((TrainingTarget[i] - (1 / (1 + np.exp(-(np.dot(np.transpose(W_Now),np.transpose(TrainingData1[i]))))))),TrainingData1[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W\n",
    "        W_Now         = W_T_Next\n",
    "\n",
    "        if user_option_3 == \"1\":\n",
    "            #-----------------TrainingData Accuracy and Erms---------------------#\n",
    "            TR_TEST_OUT   = GetValTestLinear(TRAINING_PHI,W_T_Next) \n",
    "            Erms_TR       = GetErmsLinear(TR_TEST_OUT,TrainingTarget)\n",
    "            L_Accuracy_TR.append(float(Erms_TR.split(',')[0]))\n",
    "            L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "            #-----------------ValidationData Accuracy and Erms---------------------#\n",
    "            VAL_TEST_OUT  = GetValTestLinear(VAL_PHI,W_T_Next) \n",
    "            Erms_Val      = GetErmsLinear(VAL_TEST_OUT,ValDataAct)\n",
    "            L_Accuracy_Val.append(float(Erms_Val.split(',')[0]))\n",
    "            L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "            #-----------------TestingData Accuracy and Erms---------------------#\n",
    "            TEST_OUT      = GetValTestLinear(TEST_PHI,W_T_Next) \n",
    "            Erms_Test = GetErmsLinear(TEST_OUT,TestDataAct)\n",
    "            L_Accuracy_Test.append(float(Erms_Test.split(',')[0]))\n",
    "            L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "\n",
    "        elif user_option_3 == \"2\":\n",
    "            #-----------------TrainingData Accuracy and Erms---------------------#\n",
    "            #print(TrainingData1.shape)\n",
    "            #print(W_T_Next.shape)\n",
    "            TR_TEST_OUT   = GetValTestLinear(TrainingData1,W_T_Next) \n",
    "            Erms_TR       = GetErmsLinear(TR_TEST_OUT,TrainingTarget)\n",
    "            L_Accuracy_TR.append(float(Erms_TR.split(',')[0]))\n",
    "            L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "            #-----------------ValidationData Accuracy and Erms---------------------#\n",
    "            VAL_TEST_OUT  = GetValTestLinear(np.transpose(ValData),W_T_Next) \n",
    "            Erms_Val      = GetErmsLinear(VAL_TEST_OUT,ValDataAct)\n",
    "            L_Accuracy_Val.append(float(Erms_Val.split(',')[0]))\n",
    "            L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "            #-----------------TestingData Accuracy and Erms---------------------#\n",
    "            TEST_OUT      = GetValTestLinear(np.transpose(TestData),W_T_Next) \n",
    "            Erms_Test = GetErmsLinear(TEST_OUT,TestDataAct)\n",
    "            L_Accuracy_Test.append(float(Erms_Test.split(',')[0]))\n",
    "            L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "        \n",
    "if user_option_3 == \"3\":\n",
    "\n",
    "    #print(len(TrainingTarget))\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    #print(len(processedLabel[1]))\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    model = get_model()\n",
    "    print(\"Prepared model\")\n",
    "    validation_data_split = 0.2\n",
    "    num_epochs = 10000\n",
    "    model_batch_size = 100\n",
    "    tb_batch_size = 32 \n",
    "    early_patience = 500 #tried values from 5-5000\n",
    "\n",
    "    tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True) #Tensorboard is a tool that comes \n",
    "    # integrated with tensorflow which is used in order to visualize(In the form of graphs)\n",
    "    # the changes that are taking place in the neural network over the\n",
    "    # process of learning.\n",
    "    earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min') #if the val_loss that is being\n",
    "    # monitored becomes greater than the value that it was in the previous iteration, then the training would be stopped\n",
    "    \n",
    "    processedTrainingLabel= encodeLabel(TrainingTarget)\n",
    "    print(len(processedTrainingLabel))\n",
    "    #print(len(TrainingData))\n",
    "    history = model.fit(np.transpose(TrainingData) # will tell you what signal (input data) we get\n",
    "                        , processedTrainingLabel # The output that we are expecting\n",
    "                        , validation_split=validation_data_split #a part of the training set will be used as the validation set\n",
    "                        # in order to train the hyperparameters\n",
    "                        , epochs=num_epochs #the number of times we go through the entire data set.\n",
    "                        , batch_size=model_batch_size #the model will not take all the training values at once to train the system.\n",
    "                        # it takes it in batches which is defined by the batch_size.\n",
    "                        , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                       )\n",
    "    \n",
    "    wrong   = 0\n",
    "    right   = 0\n",
    "    processedTestLabel = encodeLabel(TestDataAct)\n",
    "    predictedTestLabel = []\n",
    "    \n",
    "    for i,j in zip(np.transpose(TestData),processedTestLabel):\n",
    "        y = model.predict(np.array(i).reshape(-1,input_size))\n",
    "        predictedTestLabel.append(y.argmax())\n",
    "\n",
    "        if j.argmax() == y.argmax():\n",
    "            right = right + 1\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "    print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "    print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))\n",
    "\n",
    "\n",
    "    #print(\"Model is ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_option_3 == \"1\" or user_option_3 == \"2\":\n",
    "    print ('----------Gradient Descent Solution--------------------')\n",
    "    print (\"M = 15 \\nLambda  = 0.01\\neta=0.01\")\n",
    "    print (\"Accuracy Training   = \" + str(np.around(max(L_Accuracy_TR),5)))\n",
    "    print (\"Accuracy Validation = \" + str(np.around(max(L_Accuracy_Val),5)))\n",
    "    print (\"Accuracy Testing    = \" + str(np.around(max(L_Accuracy_Test),5)))\n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
